---
categories:  
- ""    #the front matter should be like the one found in, e.g., blog2.md. It cannot be like the normal Rmd we used
- ""
date: "2021-09-30"
description: Sample Assignment # the title that will show up once someone gets to this page
draft: false

keywords: ""
slug: sample
title: Sample Assignment
---


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(gapminder)  # gapminder dataset
library(here)
library(janitor)
```


# Task 1: `gapminder` country comparison

You have seen the `gapminder` dataset that has data on life expectancy, population, and GDP per capita for 142 countries from 1952 to 2007. To get a glimpse of the dataframe, namely to see the variable names, variable types, etc., we use the `glimpse` function. We also want to have a look at the first 20 rows of data.

```{r}
glimpse(gapminder)

head(gapminder, 20) # look at the first 20 rows of the dataframe

```

Your task is to produce two graphs of how life expectancy has changed over the years for the `country` and the `continent` you come from.

I have created the `country_data` and `continent_data` with the code below.

```{r}
country_data <- gapminder %>% 
            filter(country == "Germany") # just choosing Germany, as Ukraine (where I come from) is unavailable

continent_data <- gapminder %>% 
            filter(continent == "Europe")
```

First, create a plot of life expectancy over time for the single country you chose. Map `year` on the x-axis, and `lifeExp` on the y-axis. You should also use `geom_point()` to see the actual data points and `geom_smooth(se = FALSE)` to plot the underlying trendlines. You need to remove the comments **\#** from the lines below for your code to run.

```{r, lifeExp_one_country}
plot1 <- ggplot(data = country_data, mapping = aes(x = year, y = lifeExp))+
geom_point() +
geom_smooth(se = FALSE)+
NULL 

plot1
```

Next we need to add a title. Create a new plot, or extend plot1, using the `labs()` function to add an informative title to the plot.

```{r, lifeExp_one_country_with_label}
plot1<- plot1 +
labs(title = " Life Expectancy in Germany 1952-2007 ",
x = " Year ",
y = " Life Expectancy ") +
NULL


plot1
```

Secondly, produce a plot for all countries in the *continent* you come from. (Hint: map the `country` variable to the colour aesthetic. You also want to map `country` to the `group` aesthetic, so all points for each country are grouped together).

```{r lifeExp_one_continent}
ggplot(continent_data, mapping = aes(x = year  , y = lifeExp  , colour= country , group =country))+
geom_point() + 
geom_smooth(se = FALSE) +
NULL
```

Finally, using the original `gapminder` data, produce a life expectancy over time graph, grouped (or faceted) by continent. We will remove all legends, adding the `theme(legend.position="none")` in the end of our ggplot.

```{r lifeExp_facet_by_continent}
ggplot(data = gapminder , mapping = aes(x = year  , y = lifeExp , colour= continent ))+
geom_point() + 
geom_smooth(se = FALSE) +
facet_wrap(~continent) +
theme(legend.position="none") + #remove all legends
NULL
```

Given these trends, what can you say about life expectancy since 1952? Again, don't just say what's happening in the graph. Tell some sort of story and speculate about the differences in the patterns.

> Type your answer after this blockquote.

From history, we know that 1952 is just 7 years after World War 2, so there is still a significant effect on general life expectancy worldwide. Nevertheless, the subsequent economic recovery fostered the following increase. 

In Europe, the increase was gradual, as they already had high life expectancy levels, and although the infrastructure was destroyed after the War, the worst times were already in the past (with the help of the Marshall plan). The European countries still owned African resources and have used them to continue their economic growth in 1950-1960. In terms of variance, one outlying country only caught up with the rest of the population around 1997.

From the African standpoint, we know that the life expectancy is lowest on average, compared to the other continents, and although in 1960, a vast majority of Africans gained independence, their life expectancy increase was gradually going up, until the continent was plagued with wars starting from the 1990s. The variance in these countries was one of the lowest because of the state of medicine and its availability in the whole continent, where affordable healthcare is almost non-existent even nowadays.

For the Americas, the increase was steady, as the world's economic center was moved to North America (the USA, in particular). Hence, the countries benefited from the USA's further economic emergence and the absence of big political conflicts.

Asia is at a time of fast economic growth, as the Four Asian Tigers are emerging, drastically changing their economic condition, education, and life expectancy to a better level. The variance here was largest, as some countries had already developed a robust medical system (Japan), while the others, like Myanmar (Burma, at that time), Laos, and Cambodia, were very underdeveloped.

In Oceania, the increase was also gradual, as the medicine in Australia/New Zealand improved, and all countries avoided economic and political conflicts.

# Task 2: Brexit vote analysis

We will have a look at the results of the 2016 Brexit vote in the UK. First we read the data using `read_csv()` and have a quick glimpse at the data

```{r load_brexit_data, warning=FALSE, message=FALSE}
brexit_results <- read_csv(here::here("data","brexit_results.csv"))


glimpse(brexit_results)
```

The data comes from [Elliott Morris](https://www.thecrosstab.com/), who cleaned it and made it available through his [DataCamp class on analysing election and polling data in R](https://www.datacamp.com/courses/analyzing-election-and-polling-data-in-r).

Our main outcome variable (or y) is `leave_share`, which is the percent of votes cast in favour of Brexit, or leaving the EU. Each row is a UK [parliament constituency](https://en.wikipedia.org/wiki/United_Kingdom_Parliament_constituencies).

To get a sense of the spread, or distribution, of the data, we can plot a histogram, a density plot, and the empirical cumulative distribution function of the leave % in all constituencies.

```{r brexit_histogram, warning=FALSE, message=FALSE}

# histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_histogram(binwidth = 2.5) +
  labs(title = "Histogram of Constituencies which Voted for Brexit", 
       x = "Share of Constituency which Voted Pro Brexit (%)",
       y = "Number of Constituencies")

# density plot-- think smoothed histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_density() +
  labs(title = "Density of Constituencies which Voted for Brexit", 
       x = "Share of Constituency which Voted Pro Brexit (%)",
       y = "Fraction of Constituencies" )

# The empirical cumulative distribution function (ECDF) 
ggplot(brexit_results, aes(x = leave_share)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Empirical Cumulative Distribution for Constituencies which Voted for Brexit", 
       x = "Share of Constituency which Voted Pro Brexit (%)",
       y = "Cumulative Share of Constituencies (%)")


```

One common explanation for the Brexit outcome was fear of immigration and opposition to the EU's more open border policy. We can check the relationship (or correlation) between the proportion of native born residents (`born_in_uk`) in a constituency and its `leave_share`. To do this, let us get the correlation between the two variables

```{r brexit_immigration_correlation}
brexit_results %>% 
  select(leave_share, born_in_uk) %>% 
  cor()
```

The correlation is almost 0.5, which shows that the two variables are positively correlated.

We can also create a scatterplot between these two variables using `geom_point`. We also add the best fit line, using `geom_smooth(method = "lm")`.

```{r brexit_immigration_plot}
ggplot(brexit_results, aes(x = born_in_uk, y = leave_share)) +
  geom_point(alpha=0.3) +
  geom_smooth(method = "lm") + 
  theme_bw() +
  labs(title = "Brexit Polling Results (Consistuency)", x = "Share of UK-born residents", y = "Share of pro Brexit Voters")
  NULL
```

You have the code for the plots, I would like you to revisit all of them and use the `labs()` function to add an informative title, subtitle, and axes titles to all plots.

What can you say about the relationship shown above? Again, don't just say what's happening in the graph. Tell some sort of story and speculate about the differences in the patterns.

> Type your answer after, and outside, this blockquote.

In the data shown, we can conclude and confirm that the greater the share of UK-born residents are in the constituency, the more likely that constituency voted in favor of Brexit. In real life, this corresponded to rural areas of the UK, with a low/medium number of immigrants located in several less developed regions of the UK (North, for example). 

Although there were outliers in the election results, as there could be multiple factors affecting voting for the Brexit decision, the population origin seems to be one of the most valid arguments for why people voted for Brexit. The density of constituency was also a factor, as the more dense the population was, the more pro-brexitors were there. In real life, the vast majority of UK tiny villages and workers agglomerations in the suburbs, which are predominantly white, excluding the non-UK-born residents, voted for Brexit, again, proving the initial claim and popular opinion about why UK exited EU. Also, another factor that might have affected Brexit was the foreign population, as foreigners do cluster in large metropolitan regions, and the vast majority of Britons who voted for Brexit don't have exposure to foreigners and their cultures.

# Task 3: Animal rescue incidents attended by the London Fire Brigade

[The London Fire Brigade](https://data.london.gov.uk/dataset/animal-rescue-incidents-attended-by-lfb) attends a range of non-fire incidents (which we call 'special services'). These 'special services' include assistance to animals that may be trapped or in distress. The data is provided from January 2009 and is updated monthly. A range of information is supplied for each incident including some location information (postcode, borough, ward), as well as the data/time of the incidents. We do not routinely record data about animal deaths or injuries.

Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.

```{r load_animal_rescue_data, message=FALSE, warning=FALSE}

url <- "https://data.london.gov.uk/download/animal-rescue-incidents-attended-by-lfb/8a7d91c2-9aec-4bde-937a-3998f4717cd8/Animal%20Rescue%20incidents%20attended%20by%20LFB%20from%20Jan%202009.csv"

animal_rescue <- read_csv(url,
                          locale = locale(encoding = "CP1252")) %>% 
  janitor::clean_names()


glimpse(animal_rescue)
```
One of the more useful things one can do with any data set is quick counts, namely to see how many observations fall within one category. For instance, if we wanted to count the number of incidents by year, we would either use `group_by()... summarise()` or, simply [`count()`](https://dplyr.tidyverse.org/reference/count.html)

```{r, instances_by_calendar_year}

animal_rescue %>% 
  dplyr::group_by(cal_year) %>% 
  summarise(count=n())

animal_rescue %>% 
  count(cal_year, name="count")

```

Let us try to see how many incidents we have by animal group. Again, we can do this either using group_by() and summarise(), or by using count()

```{r, animal_group_percentages}
animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  
  #group_by and summarise will produce a new column with the count in each animal group
  summarise(count = n()) %>% 
  
  # mutate adds a new column; here we calculate the percentage
  mutate(percent = round(100*count/sum(count),2)) %>% 
  
  # arrange() sorts the data by percent. Since the default sorting is min to max and we would like to see it sorted
  # in descending order (max to min), we use arrange(desc()) 
  arrange(desc(percent))


animal_rescue %>% 
  
  #count does the same thing as group_by and summarise
  # name = "count" will call the column with the counts "count" ( exciting, I know)
  # and 'sort=TRUE' will sort them from max to min
  count(animal_group_parent, name="count", sort=TRUE) %>% 
  mutate(percent = round(100*count/sum(count),2))


```

Do you see anything strange in these tables? Around 84% of all the incidents are applying to only 3 animal groups (Cat,Bird,Dog).Also, there were some surprising animals groups, such as tortoise and hedgehod.

Finally, let us have a loot at the notional cost for rescuing each of these animals. As the LFB says,

> Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.

There is two things we will do:

1. Calculate the mean and median `incident_notional_cost` for each `animal_group_parent`
2. Plot a boxplot to get a feel for the distribution of `incident_notional_cost` by `animal_group_parent`.


Before we go on, however, we need to fix `incident_notional_cost` as it is stored as a `chr`, or character, rather than a number.

```{r, parse_incident_cost,message=FALSE, warning=FALSE}

# what type is variable incident_notional_cost from dataframe `animal_rescue`
typeof(animal_rescue$incident_notional_cost)

# readr::parse_number() will convert any numerical values stored as characters into numbers
animal_rescue <- animal_rescue %>% 

  # we use mutate() to use the parse_number() function and overwrite the same variable
  mutate(incident_notional_cost = parse_number(incident_notional_cost))

# incident_notional_cost from dataframe `animal_rescue` is now 'double' or numeric
typeof(animal_rescue$incident_notional_cost)

```

Now tht incident_notional_cost is numeric, let us quickly calculate summary statistics for each animal group. 


```{r, stats_on_incident_cost,message=FALSE, warning=FALSE}

animal_rescue %>% 
  
  # group by animal_group_parent
  group_by(animal_group_parent) %>% 
  
  # filter resulting data, so each group has at least 6 observations
  filter(n()>6) %>% 
  
  # summarise() will collapse all values into 3 values: the mean, median, and count  
  # we use na.rm=TRUE to make sure we remove any NAs, or cases where we do not have the incident cos
  summarise(mean_incident_cost = mean (incident_notional_cost, na.rm=TRUE),
            median_incident_cost = median (incident_notional_cost, na.rm=TRUE),
            sd_incident_cost = sd (incident_notional_cost, na.rm=TRUE),
            min_incident_cost = min (incident_notional_cost, na.rm=TRUE),
            max_incident_cost = max (incident_notional_cost, na.rm=TRUE),
            count = n()) %>% 
  
  # sort the resulting data in descending order. You choose whether to sort by count or mean cost.
  arrange(desc(animal_group_parent))

```


Compare the mean and the median for each animal group. 

1. Unknown - Wild Animal mean>median
2. Unknown - Heavy Livestock Aminal mean>median
3. Unknown - Domestic Animal Or Pet mean>median
4. Squirrel mean<median
5. Snake mean>median
6. Rabbit mean<median
7. Horse mean>median
8. Hamster mean>median
9. Fox mean>median
10. Ferret mean<median
11. Dog mean>median
12. Deer mean>median
13. Cow mean>median
14. Cat mean>median
15. cat mean>median
16. Bird mean>median

What do you think this is telling us? For most animal groups, the mean is greater than the median, which leads to a right-skewed distribution. Only for three groups: Squirrel, Ferret, and Rabbit, the distribution is left-skewed. We can also conclude that right-skewed groups of animals are domestic, while left-skewed are wild ones. Also, surprisingly, the Snake is right-skewed and it is a wild animal. 

Anything else that stands out? Any outliers? For most animal groups, there are a few very costly incidents, as the data is skewed to the right. The minimum incident cost is closer to the median for the majority of the animal groups, except Cow and Horse (where the median is more than twice as minimal). A possible reason for that is that the animal's size is related to the incident cost, as both Cow and Horse have the highest median cost among the data.

Finally, let us plot a few plots that show the distribution of incident_cost for each animal group.

```{r, plots_on_incident_cost_by_animal_group,message=FALSE, warning=FALSE}

# base_plot
base_plot <- animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  filter(n()>6) %>% 
  ggplot(aes(x=incident_notional_cost))+
  facet_wrap(~animal_group_parent, scales = "free")+
  theme_bw()

base_plot + geom_histogram()
base_plot + geom_density()
base_plot + geom_boxplot()
base_plot + stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent)



```

Which of these four graphs do you think best communicates the variability of the `incident_notional_cost` values? Also, can you please tell some sort of story (which animals are more expensive to rescue than others, the spread of values) and speculate about the differences in the patterns.

In my opinion, the histogram graph best communicates the variability of incident cost. With the help of this graph, we can see how frequent and how distributed the incident costs are. The other three options also provide a decent overview of the data shown, but the histogram still summarizes it better. Also, given the number of the animal groups (16), the histogram is, in my opinion, the most accessible chart to comprehend. 

The data presented shows that the big animals, such as horses, cows, deers, and heavy livestock, have large notional incident cost amounts. The injuries related to these accidents could cost more due to their size and weight. The cost values in animal groups are also drastically spread, as there is more significant variability in the accident costs. From my personal experience, large livestock does require serious medical attention if injured. Also, from the information presented, we can conclude that the size of the animals is one of the most significant predictors of incident cost, and there is a difference in cost between the incidents with domestic and wild animals.

# Submit the assignment

Knit the completed R Markdown file as an HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas.

## Details

If you want to, please answer the following

-   Who did you collaborate with: Tomas Arrarte Raffo, Akos Ersek
-   Approximately how much time did you spend on this problem set: 8 hours
-   What, if anything, gave you the most trouble: Labeling, exercise 3.
